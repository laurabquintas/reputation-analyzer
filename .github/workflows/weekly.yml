name: Weekly Reputation Update

on:
  schedule:
    - cron: "0 6 * * 1" # Mondays 06:00 UTC
  workflow_dispatch:

jobs:
  test:
    runs-on: self-hosted
    permissions:
      contents: read
    steps:
      - uses: actions/checkout@v4

      - name: Show Python version on self-hosted runner
        run: python3 --version

      - name: Install deps
        run: |
          python3 -m pip install -U pip
          python3 -m pip install -r requirements.txt
          python3 -m pip install pytest

      - name: Run unit tests
        run: python3 -m pytest -q

  scrape-all:
    runs-on: self-hosted
    needs: test
    permissions:
      contents: read
    outputs:
      scrape_outcome: ${{ steps.scrape.outcome }}
      has_issues: ${{ steps.evaluate.outputs.has_issues }}
      issue_count: ${{ steps.evaluate.outputs.issue_count }}
    steps:
      - uses: actions/checkout@v4

      - name: Show Python version on self-hosted runner
        run: python3 --version

      - name: Install deps
        run: |
          python3 -m pip install -U pip
          python3 -m pip install -r requirements.txt

      - name: Run weekly scraping
        id: scrape
        continue-on-error: true
        env:
          GOOGLE_MAPS_API_KEY: ${{ secrets.GOOGLE_MAPS_API_KEY }}
          GOOGLE_PLACES_API_KEY: ${{ secrets.GOOGLE_PLACES_API_KEY }}
          TRIPADVISOR_API_KEY: ${{ secrets.TRIPADVISOR_API_KEY }}
        run: |
          python3 -m src.run --summary-json data/run_summary.json

      - name: Upload run summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: run-summary
          path: data/run_summary.json
          if-no-files-found: warn

      - name: Evaluate run summary
        id: evaluate
        if: always()
        run: |
          python3 - <<'PY'
          import json
          from pathlib import Path

          p = Path("data/run_summary.json")
          has_issues = False
          issue_count = 0
          if p.exists():
            data = json.loads(p.read_text(encoding="utf-8"))
            results = data.get("results", [])
            issue_count = sum(1 for r in results if str(r.get("status", "")).lower() != "ok")
            has_issues = issue_count > 0

          with open("${GITHUB_OUTPUT}", "a", encoding="utf-8") as fh:
            fh.write(f"has_issues={'true' if has_issues else 'false'}\n")
            fh.write(f"issue_count={issue_count}\n")
          PY

  commit-data:
    if: always()
    runs-on: self-hosted
    needs: scrape-all
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download run summary
        if: always()
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: run-summary
          path: data

      - name: Commit updated CSV files
        run: |
          if git diff --quiet -- data/*.csv; then
            echo "No CSV changes to commit."
            exit 0
          fi

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/*.csv
          git commit -m "chore(data): weekly score update $(date -u +%F)"
          git push

      - name: Send alert email on issues
        if: needs['scrape-all'].outputs.has_issues == 'true'
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          subject: "[Reputation Analyzer] Workflow issues detected"
          to: ${{ secrets.ALERT_EMAIL_TO }}
          from: ${{ secrets.ALERT_EMAIL_FROM }}
          secure: true
          body: |
            Weekly workflow finished with issues.
            Issue count: ${{ needs['scrape-all'].outputs.issue_count }}
            Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          attachments: data/run_summary.json

      - name: Mark workflow failed if scraping failed
        if: needs['scrape-all'].outputs.scrape_outcome == 'failure'
        run: |
          echo "One or more scrapers failed. Partial CSV updates were still committed."
          exit 1
